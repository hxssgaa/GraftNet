{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import wordninja\n",
    "\n",
    "from pullnet import PullNet\n",
    "from graftnet import GraftNet\n",
    "from pullnet_data_loader import DataLoader\n",
    "from fpnet_data_loader import FpNetDataLoader\n",
    "from relreasoner_data_loader import RelReasonerDataLoader\n",
    "from fpnet import FactsPullNet\n",
    "from util import *\n",
    "from multiprocessing.pool import Pool\n",
    "from preprocessing import use_helper\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config('config/relreasoner_order_metaqa3.yml')\n",
    "facts = load_fact2(cfg['fact_data'])\n",
    "facts_rel = load_fact(cfg['fact_data'])\n",
    "word2id = load_dict(cfg['data_folder'] + cfg['word2id'])\n",
    "relation2id = load_dict(cfg['data_folder'] + cfg['relation2id'])\n",
    "entity2id = load_dict(cfg['data_folder'] + cfg['entity2id'])\n",
    "num_hop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "118980it [00:01, 92576.94it/s]\n",
      "14872it [00:00, 179883.58it/s]\n",
      "14872it [00:00, 178979.82it/s]\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_FOLDER = 'datasets/metaqa/'\n",
    "TRAIN_FILE = 'qa_train.txt'\n",
    "DEV_FILE = 'qa_dev.txt'\n",
    "TEST_FILE = 'qa_test.txt'\n",
    "KB_FILE = 'kb.txt'\n",
    "HOP = 2\n",
    "\n",
    "def process_questions(kb, hop=1):\n",
    "    question_files = [(TRAIN_FILE, 'train'), (DEV_FILE, 'dev'), (TEST_FILE, 'test')]\n",
    "    for q_file, q_type in question_files:\n",
    "        qa_datas = []\n",
    "        with open(os.path.join(BASE_DATA_FOLDER, '%dhop' % hop, q_file)) as f:\n",
    "            qas = f.readlines()\n",
    "            for idx, qa in tqdm(enumerate(qas)):\n",
    "                question, answers = tuple(map(str.strip, qa.split('\\t')))\n",
    "                answers = list(map(str.strip, answers.split('|')))\n",
    "                topic_entities = list(map(str.strip, ''.join(question[question.index('[') + 1: question.rindex(']')]).split('|')))\n",
    "                qa_data = {\n",
    "                    'question': question.replace('[', '').replace(']', '') + ' ?',\n",
    "                    'answers': list(map(lambda x: {\n",
    "                        'text': x\n",
    "                    }, answers)),\n",
    "                    'entities': list(map(lambda x: {\n",
    "                        'text': x,\n",
    "                        'kb_id': x\n",
    "                    }, topic_entities)),\n",
    "                    'id': '%s_%d' % (q_type, idx),\n",
    "                }\n",
    "                qa_datas.append(qa_data)\n",
    "        save_json(qa_datas, os.path.join(BASE_DATA_FOLDER, '%dhop' % hop, '%s.json' % q_type))\n",
    "                \n",
    "process_questions(facts, hop=HOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(cfg['data_folder'] + cfg['train_data'])\n",
    "dev_data = load_json(cfg['data_folder'] + cfg['dev_data'])\n",
    "test_data = load_json(cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg['data_folder'] + 'qa_train_qtype.txt') as f:\n",
    "    train_qtypes = f.readlines()\n",
    "with open(cfg['data_folder'] + 'qa_dev_qtype.txt') as f:\n",
    "    dev_qtypes = f.readlines()\n",
    "with open(cfg['data_folder'] + 'qa_test_qtype.txt') as f:\n",
    "    test_qtypes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118980"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_qtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118980/118980 [00:00<00:00, 234225.62it/s]\n",
      "100%|██████████| 14872/14872 [00:00<00:00, 526580.64it/s]\n",
      "100%|██████████| 14872/14872 [00:00<00:00, 519485.07it/s]\n"
     ]
    }
   ],
   "source": [
    "REL_MAPPING = {\n",
    "    'movie': {\n",
    "        'actor': 'starred_actors',\n",
    "        'writer': 'written_by',\n",
    "        'director': 'directed_by',\n",
    "        'language': 'in_language',\n",
    "        'year': 'release_year',\n",
    "        'genre': 'has_genre',\n",
    "        'tags': 'has_tags',\n",
    "        'tag': 'has_tags',\n",
    "        'imdbrating': 'has_imdb_rating',\n",
    "        'imdbvotes': 'has_imdb_votes',\n",
    "    }\n",
    "}\n",
    "tmp = defaultdict(dict)\n",
    "for k1, v1 in REL_MAPPING.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        tmp[k1][k2] = v2\n",
    "        tmp[k2][k1] = v2\n",
    "REL_MAPPING = tmp\n",
    "for qtypes_data in [(train_qtypes, 'train'), (dev_qtypes, 'dev'), (test_qtypes, 'test')]:\n",
    "    qtypes, data_type = qtypes_data\n",
    "    for idx_e, e in enumerate(tqdm(qtypes)):\n",
    "        e = e.strip()\n",
    "        ts = e.split('_')\n",
    "        rels = []\n",
    "        for idx in range(0, len(ts) - 2, 2):\n",
    "            rels.append(REL_MAPPING[ts[idx]][ts[idx+2]])\n",
    "        if data_type == 'train':    \n",
    "            train_data[idx_e]['rel_path'] = rels\n",
    "        elif data_type == 'dev':\n",
    "            dev_data[idx_e]['rel_path'] = rels\n",
    "        else:\n",
    "            test_data[idx_e]['rel_path'] = rels\n",
    "save_json(train_data, cfg['data_folder'] + cfg['train_data'])\n",
    "save_json(dev_data, cfg['data_folder'] + cfg['dev_data'])\n",
    "save_json(test_data, cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what languages are the films starred by Simone Simon in ?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['starred_actors', 'in_language']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]['rel_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directed_by': {'Anatole Litvak': 0, 'Don Bluth': 0, 'Gary Goldman': 0},\n",
       " 'written_by': {'Guy Bolton': 0, 'Marcelle Maurette': 0},\n",
       " 'starred_actors': {'Ingrid Bergman': 0,\n",
       "  'Yul Brynner': 0,\n",
       "  'Helen Hayes': 0,\n",
       "  'John Cusack': 0,\n",
       "  'Christopher Lloyd': 0,\n",
       "  'Meg Ryan': 0,\n",
       "  'Kelsey Grammer': 0},\n",
       " 'release_year': {'1956': 0, '1997': 0},\n",
       " 'has_genre': {'Drama': 0, 'Animation': 0},\n",
       " 'has_tags': {'bd-r': 0,\n",
       "  'ingrid bergman': 0,\n",
       "  'yul brynner': 0,\n",
       "  'anatole litvak': 0,\n",
       "  'helen hayes': 0,\n",
       "  'animation': 0,\n",
       "  'music': 0,\n",
       "  'kirsten dunst': 0,\n",
       "  'meg ryan': 0,\n",
       "  'christopher lloyd': 0,\n",
       "  'russia': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_rel['Anastasia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_helper(entity, answers, num_hop, tmp_path, rel_path, visited):\n",
    "    if entity in visited:\n",
    "        return set()\n",
    "    if num_hop == 0:\n",
    "        if entity in answers:\n",
    "            return {tuple(tmp_path)}\n",
    "        return set()\n",
    "    visited.add(entity)\n",
    "    res = set()\n",
    "    entity = entity.replace('%', '')\n",
    "    cur_rel = rel_path[len(rel_path) - num_hop]\n",
    "    if cur_rel not in facts_rel[entity]:\n",
    "        return set()\n",
    "    for k2 in facts_rel[entity][cur_rel]:\n",
    "        v = find_paths_helper(k2, answers, num_hop - 1, tmp_path + [k2], rel_path, visited)\n",
    "        res.update(v)\n",
    "    return res\n",
    "\n",
    "def find_paths(questions, num_hop):\n",
    "    res = []\n",
    "    for e in tqdm(questions):\n",
    "        visited = set()\n",
    "        answers = set(map(lambda x: x['text'], e['answers']))\n",
    "        entity = e['entities'][0]['text']\n",
    "        v = find_paths_helper(entity, answers, num_hop, [entity], e['rel_path'], visited)\n",
    "        e['path'] = list(v)\n",
    "        res.append(e)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118980/118980 [00:01<00:00, 92007.01it/s]\n",
      "100%|██████████| 14872/14872 [00:00<00:00, 90057.10it/s]\n",
      "100%|██████████| 14872/14872 [00:00<00:00, 92832.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = find_paths(train_data, num_hop)\n",
    "dev_data = find_paths(dev_data, num_hop)\n",
    "test_data = find_paths(test_data, num_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(train_data, cfg['data_folder'] + cfg['train_data'])\n",
    "save_json(dev_data, cfg['data_folder'] + cfg['dev_data'])\n",
    "save_json(test_data, cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for e in train_data:\n",
    "    lens.append(len(set(e['rel_path'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9893\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, filter_dot=False):\n",
    "    text = text.replace('.', ' . ').lower()\n",
    "    for punc in punctuation:\n",
    "        if punc != '.':\n",
    "            text = text.replace(punc, \" \")\n",
    "    text = text.split()\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if len(i) < 10:\n",
    "            output.append(i)\n",
    "        else:\n",
    "            output.extend(wordninja.split(i))\n",
    "    if filter_dot:\n",
    "        return [e for e in text if e != '.']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14274/14274 [00:00<00:00, 57325.67it/s]\n"
     ]
    }
   ],
   "source": [
    "total_words = set()\n",
    "for q in tqdm(dev_data):\n",
    "    cleaned = clean_text(q['question'])\n",
    "    total_words.update(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = set()\n",
    "for rel in relation2id:\n",
    "    cleaned = clean_text(rel)\n",
    "    total_words.update(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(total_words & set(word2id.keys())) / len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actors',\n",
       " 'by',\n",
       " 'directed',\n",
       " 'genre',\n",
       " 'has',\n",
       " 'imdb',\n",
       " 'in',\n",
       " 'language',\n",
       " 'rating',\n",
       " 'release',\n",
       " 'starred',\n",
       " 'tags',\n",
       " 'votes',\n",
       " 'written',\n",
       " 'year'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = set(word2id.keys()) | total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/metaqa/3hop/vocab_new.txt', 'w') as f:\n",
    "    for e in new_words:\n",
    "        f.writelines(e + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the films that share actors with the film Dil Chahta Hai were released in which years ?',\n",
       " ['starred_actors', 'starred_actors', 'release_year'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "train_data[idx]['question'], train_data[idx]['rel_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_imdb_rating': 0,\n",
       " 'starred_actors': 1,\n",
       " 'in_language': 2,\n",
       " 'has_imdb_votes': 3,\n",
       " 'has_tags': 4,\n",
       " 'written_by': 5,\n",
       " 'directed_by': 6,\n",
       " 'has_genre': 7,\n",
       " 'release_year': 8}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt = use_helper.UseVector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for initialize graph:  1.2292625904083252\n",
      "Time for init session:  1.1187412738800049\n"
     ]
    }
   ],
   "source": [
    "rel_emb = {r: cvt.get_vector(r) for r in relation2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 91.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'has_genre', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'starred_actors'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'has_tags', 'directed_by'} ,answers: {'in_language', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'starred_actors'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "0.4090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall = 0.0\n",
    "total = 0\n",
    "for idx in tqdm(range(len(train_data)//10000)):\n",
    "    q_emb = cvt.get_vector(train_data[idx]['question'])\n",
    "    rel2score = dict()\n",
    "    for rel, remb in rel_emb.items():\n",
    "        qemb = torch.tensor(q_emb)\n",
    "        remb = torch.tensor(remb)\n",
    "        rel2score[rel] = torch.sigmoid(remb @ qemb.T).item()\n",
    "    rels = set(map(lambda x: x[0], list(sorted(rel2score.items(), key=lambda x: x[1], reverse=True)[:2])))\n",
    "    answers = set(train_data[idx]['rel_path'])\n",
    "    recall += len(rels & answers) / len(answers)\n",
    "    total += 1\n",
    "    print('rel:', rels, ',answers:', answers)\n",
    "print(recall / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = len(word2id)\n",
    "word_dim = 100\n",
    "word_embedding = nn.Embedding(num_embeddings=num_word + 1, embedding_dim=word_dim, padding_idx=num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.weight = nn.Parameter(\n",
    "                torch.from_numpy(np.pad(np.load('datasets/metaqa/3hop/word_emb_100d.npy'), ((0, 1), (0, 0)), 'constant')).type(\n",
    "                    'torch.FloatTensor'))\n",
    "word_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the films that share actors with the film Dil Chahta Hai were released in which years ?\n",
      "['starred_actors', 'starred_actors', 'release_year']\n",
      "has_imdb_rating\n",
      "starred_actors\n",
      "in_language\n",
      "has_imdb_votes\n",
      "has_tags\n",
      "written_by\n",
      "directed_by\n",
      "has_genre\n",
      "release_year\n",
      "rel: has_imdb_rating , similarity: 0.6172443628311157\n",
      "rel: starred_actors , similarity: 0.9318690896034241\n",
      "rel: in_language , similarity: 0.6172443628311157\n",
      "rel: has_imdb_votes , similarity: 0.6172443628311157\n",
      "rel: has_tags , similarity: 0.6172443628311157\n",
      "rel: written_by , similarity: 0.9073621034622192\n",
      "rel: directed_by , similarity: 0.9525083899497986\n",
      "rel: has_genre , similarity: 0.6172443628311157\n",
      "rel: release_year , similarity: 0.9408514499664307\n"
     ]
    }
   ],
   "source": [
    "idx_q = 0\n",
    "q = train_data[idx_q]\n",
    "print(q['question'])\n",
    "print(q['rel_path'])\n",
    "for r in relation2id:\n",
    "    print(r)\n",
    "rels = list(relation2id.keys())\n",
    "rel_emb = word_embedding\n",
    "for rel in rels:\n",
    "    rel_spt = clean_text(rel)\n",
    "    rel_spt = list(filter(lambda x: x not in ('has', 'by', 'in', 'the'), rel_spt))\n",
    "    q_spt = clean_text(q['question'])\n",
    "    q_spt = list(filter(lambda x: x not in ('has', 'which', 'by', 'with', 'were', 'that', 'in', 'the'), q_spt))\n",
    "    similarity = []\n",
    "    rel_emb = []\n",
    "    for rel_word in rel_spt:\n",
    "        if rel_word in word2id:\n",
    "            rel_word_emb = word_embedding(torch.tensor(word2id[rel_word], dtype=torch.int64))\n",
    "        else:\n",
    "            rel_word_emb = word_embedding(torch.tensor(word2id['__unk__'], dtype=torch.int64))\n",
    "        rel_emb.append(rel_word_emb)\n",
    "    rel_emb = sum(rel_emb) / len(rel_emb)\n",
    "    for word in q_spt:\n",
    "        word_emb = word_embedding(torch.tensor(word2id[word], dtype=torch.int64))\n",
    "        similarity.append(torch.sigmoid(rel_emb @ word_emb.T / np.sqrt(word_dim)).item())\n",
    "#         print('%s <--> %s, similarity: %.8f' % (word, rel_word, similarity[-1]))\n",
    "    print('rel:', rel, ', similarity:', np.max(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_imdb_rating': 0,\n",
       " 'starred_actors': 1,\n",
       " 'in_language': 2,\n",
       " 'has_imdb_votes': 3,\n",
       " 'has_tags': 4,\n",
       " 'written_by': 5,\n",
       " 'directed_by': 6,\n",
       " 'has_genre': 7,\n",
       " 'release_year': 8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3\n",
    "for q in train_data + dev_data + test_data:\n",
    "    q['ID'] = q['id']\n",
    "    answers = q['answers']\n",
    "    if answers and isinstance(answers[0], dict):\n",
    "        answers = list(map(lambda x: x['text'], answers))\n",
    "    entities = q['entities']\n",
    "    if entities and isinstance(entities[0], dict):\n",
    "        entities = list(map(lambda x: x['kb_id'], entities))\n",
    "    \n",
    "    rel_path = q['rel_path']\n",
    "    rel_chain_map = dict()\n",
    "    for hop in range(1, T+1):\n",
    "        rel_chain_map[hop] = dict()\n",
    "        for entity in entities:\n",
    "            rel_chain_map[hop][entity] = dict()\n",
    "            ground_truth = [tuple(rel_path[:hop])]\n",
    "            cands = list(map(lambda x: tuple(list(ground_truth[0])[:-1] + [x]), list(relation2id.keys())))\n",
    "            rel_chain_map[hop][entity]['ground_truth'] = ground_truth\n",
    "            rel_chain_map[hop][entity]['cands'] = cands\n",
    "    q['entities'] = entities\n",
    "    q['answers'] = answers\n",
    "    q['rel_chain_map'] = rel_chain_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'when did the movies release whose writers also wrote Parineeta ?',\n",
       " 'answers': ['2009'],\n",
       " 'entities': ['Parineeta'],\n",
       " 'id': 'test_14273',\n",
       " 'type': 'test',\n",
       " 'path': [['Parineeta', 'Vidhu Vinod Chopra', '3 Idiots', '2009']],\n",
       " 'rel_path': ['written_by', 'written_by', 'release_year'],\n",
       " 'ID': 'test_14273',\n",
       " 'rel_chain_map': {1: {'Parineeta': {'ground_truth': [('written_by',)],\n",
       "    'cands': [('has_imdb_rating',),\n",
       "     ('starred_actors',),\n",
       "     ('in_language',),\n",
       "     ('has_imdb_votes',),\n",
       "     ('has_tags',),\n",
       "     ('written_by',),\n",
       "     ('directed_by',),\n",
       "     ('has_genre',),\n",
       "     ('release_year',)]}},\n",
       "  2: {'Parineeta': {'ground_truth': [('written_by', 'written_by')],\n",
       "    'cands': [('written_by', 'has_imdb_rating'),\n",
       "     ('written_by', 'starred_actors'),\n",
       "     ('written_by', 'in_language'),\n",
       "     ('written_by', 'has_imdb_votes'),\n",
       "     ('written_by', 'has_tags'),\n",
       "     ('written_by', 'written_by'),\n",
       "     ('written_by', 'directed_by'),\n",
       "     ('written_by', 'has_genre'),\n",
       "     ('written_by', 'release_year')]}},\n",
       "  3: {'Parineeta': {'ground_truth': [('written_by',\n",
       "      'written_by',\n",
       "      'release_year')],\n",
       "    'cands': [('written_by', 'written_by', 'has_imdb_rating'),\n",
       "     ('written_by', 'written_by', 'starred_actors'),\n",
       "     ('written_by', 'written_by', 'in_language'),\n",
       "     ('written_by', 'written_by', 'has_imdb_votes'),\n",
       "     ('written_by', 'written_by', 'has_tags'),\n",
       "     ('written_by', 'written_by', 'written_by'),\n",
       "     ('written_by', 'written_by', 'directed_by'),\n",
       "     ('written_by', 'written_by', 'has_genre'),\n",
       "     ('written_by', 'written_by', 'release_year')]}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(train_data, cfg['data_folder'] + 'train_new.json')\n",
    "save_json(dev_data, cfg['data_folder'] + 'dev_new.json')\n",
    "save_json(test_data, cfg['data_folder'] + 'test_new.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = dict()\n",
    "with open('datasets/metaqa/kb.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = list(map(lambda x: x.strip().split('|'), lines))\n",
    "for e in lines:\n",
    "    if e[0] not in facts:\n",
    "        facts[e[0]] = dict()\n",
    "    if e[1] not in facts[e[0]]:\n",
    "        facts[e[0]][e[1]] = dict()\n",
    "    if e[2] not in facts[e[0]][e[1]]:\n",
    "        facts[e[0]][e[1]][e[2]] = 0\n",
    "    if e[2] not in facts:\n",
    "        facts[e[2]] = dict()\n",
    "    if e[1] not in facts[e[2]]:\n",
    "        facts[e[2]][e[1]] = dict()\n",
    "    if e[0] not in facts[e[2]][e[1]]:\n",
    "        facts[e[2]][e[1]][e[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(facts, 'datasets/metaqa/3hop/facts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
