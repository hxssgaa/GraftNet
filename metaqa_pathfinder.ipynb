{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hxssg1124/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import wordninja\n",
    "\n",
    "from pullnet import PullNet\n",
    "from graftnet import GraftNet\n",
    "from pullnet_data_loader import DataLoader\n",
    "from fpnet_data_loader import FpNetDataLoader\n",
    "from relreasoner_data_loader import RelReasonerDataLoader\n",
    "from fpnet import FactsPullNet\n",
    "from util import *\n",
    "from multiprocessing.pool import Pool\n",
    "from preprocessing import use_helper\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config('config/relreasoner_metaqa3.yml')\n",
    "facts = load_fact2(cfg['fact_data'])\n",
    "facts_rel = load_fact(cfg['fact_data'])\n",
    "word2id = load_dict(cfg['data_folder'] + cfg['word2id'])\n",
    "relation2id = load_dict(cfg['data_folder'] + cfg['relation2id'])\n",
    "entity2id = load_dict(cfg['data_folder'] + cfg['entity2id'])\n",
    "num_hop = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_json(cfg['data_folder'] + cfg['train_data'])\n",
    "dev_data = load_json(cfg['data_folder'] + cfg['dev_data'])\n",
    "test_data = load_json(cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load question types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg['data_folder'] + 'qa_train_qtype.txt') as f:\n",
    "    train_qtypes = f.readlines()\n",
    "with open(cfg['data_folder'] + 'qa_dev_qtype.txt') as f:\n",
    "    dev_qtypes = f.readlines()\n",
    "with open(cfg['data_folder'] + 'qa_test_qtype.txt') as f:\n",
    "    test_qtypes = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114196/114196 [00:00<00:00, 430203.57it/s]\n",
      "100%|██████████| 14274/14274 [00:00<00:00, 488938.12it/s]\n",
      "100%|██████████| 14274/14274 [00:00<00:00, 503545.08it/s]\n"
     ]
    }
   ],
   "source": [
    "REL_MAPPING = {\n",
    "    'movie': {\n",
    "        'actor': 'starred_actors',\n",
    "        'writer': 'written_by',\n",
    "        'director': 'directed_by',\n",
    "        'language': 'in_language',\n",
    "        'year': 'release_year',\n",
    "        'genre': 'has_genre'\n",
    "    }\n",
    "}\n",
    "tmp = defaultdict(dict)\n",
    "for k1, v1 in REL_MAPPING.items():\n",
    "    for k2, v2 in v1.items():\n",
    "        tmp[k1][k2] = v2\n",
    "        tmp[k2][k1] = v2\n",
    "REL_MAPPING = tmp\n",
    "for qtypes_data in [(train_qtypes, 'train'), (dev_qtypes, 'dev'), (test_qtypes, 'test')]:\n",
    "    qtypes, data_type = qtypes_data\n",
    "    for idx_e, e in enumerate(tqdm(qtypes)):\n",
    "        e = e.strip()\n",
    "        ts = e.split('_')\n",
    "        rels = []\n",
    "        for idx in range(0, len(ts) - 2, 2):\n",
    "            rels.append(REL_MAPPING[ts[idx]][ts[idx+2]])\n",
    "        if data_type == 'train':    \n",
    "            train_data[idx_e]['rel_path'] = rels\n",
    "        elif data_type == 'dev':\n",
    "            dev_data[idx_e]['rel_path'] = rels\n",
    "        else:\n",
    "            test_data[idx_e]['rel_path'] = rels\n",
    "save_json(train_data, cfg['data_folder'] + cfg['train_data'])\n",
    "save_json(dev_data, cfg['data_folder'] + cfg['dev_data'])\n",
    "save_json(test_data, cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who wrote films that share actors with the film Anastasia ?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['starred_actors', 'starred_actors', 'written_by']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]['rel_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directed_by': {'Anatole Litvak': 0, 'Don Bluth': 0, 'Gary Goldman': 0},\n",
       " 'written_by': {'Guy Bolton': 0, 'Marcelle Maurette': 0},\n",
       " 'starred_actors': {'Ingrid Bergman': 0,\n",
       "  'Yul Brynner': 0,\n",
       "  'Helen Hayes': 0,\n",
       "  'John Cusack': 0,\n",
       "  'Christopher Lloyd': 0,\n",
       "  'Meg Ryan': 0,\n",
       "  'Kelsey Grammer': 0},\n",
       " 'release_year': {'1956': 0, '1997': 0},\n",
       " 'has_genre': {'Drama': 0, 'Animation': 0},\n",
       " 'has_tags': {'bd-r': 0,\n",
       "  'ingrid bergman': 0,\n",
       "  'yul brynner': 0,\n",
       "  'anatole litvak': 0,\n",
       "  'helen hayes': 0,\n",
       "  'animation': 0,\n",
       "  'music': 0,\n",
       "  'kirsten dunst': 0,\n",
       "  'meg ryan': 0,\n",
       "  'christopher lloyd': 0,\n",
       "  'russia': 0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_rel['Anastasia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paths_helper(entity, answers, num_hop, tmp_path, rel_path, visited):\n",
    "    if entity in visited:\n",
    "        return set()\n",
    "    if num_hop == 0:\n",
    "        if entity in answers:\n",
    "            return {tuple(tmp_path)}\n",
    "        return set()\n",
    "    visited.add(entity)\n",
    "    res = set()\n",
    "    entity = entity.replace('%', '')\n",
    "    cur_rel = rel_path[len(rel_path) - num_hop]\n",
    "    if cur_rel not in facts_rel[entity]:\n",
    "        return set()\n",
    "    for k2 in facts_rel[entity][cur_rel]:\n",
    "        v = find_paths_helper(k2, answers, num_hop - 1, tmp_path + [k2], rel_path, visited)\n",
    "        res.update(v)\n",
    "    return res\n",
    "\n",
    "def find_paths(questions):\n",
    "    res = []\n",
    "    for e in tqdm(questions):\n",
    "        visited = set()\n",
    "        answers = set(map(lambda x: x['text'], e['answers']))\n",
    "        entity = e['entities'][0]['text']\n",
    "        v = find_paths_helper(entity, answers, 3, [entity], e['rel_path'], visited)\n",
    "        e['path'] = list(v)\n",
    "        res.append(e)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114196/114196 [00:04<00:00, 24765.07it/s]\n",
      "100%|██████████| 14274/14274 [00:01<00:00, 12711.95it/s]\n",
      "100%|██████████| 14274/14274 [00:00<00:00, 23527.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = find_paths(train_data)\n",
    "dev_data = find_paths(dev_data)\n",
    "test_data = find_paths(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(train_data, cfg['data_folder'] + cfg['train_data'])\n",
    "save_json(dev_data, cfg['data_folder'] + cfg['dev_data'])\n",
    "save_json(test_data, cfg['data_folder'] + cfg['test_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = []\n",
    "for e in train_data:\n",
    "    lens.append(len(set(e['rel_path'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9893\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, filter_dot=False):\n",
    "    text = text.replace('.', ' . ').lower()\n",
    "    for punc in punctuation:\n",
    "        if punc != '.':\n",
    "            text = text.replace(punc, \" \")\n",
    "    text = text.split()\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if len(i) < 10:\n",
    "            output.append(i)\n",
    "        else:\n",
    "            output.extend(wordninja.split(i))\n",
    "    if filter_dot:\n",
    "        return [e for e in text if e != '.']\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14274/14274 [00:00<00:00, 57325.67it/s]\n"
     ]
    }
   ],
   "source": [
    "total_words = set()\n",
    "for q in tqdm(dev_data):\n",
    "    cleaned = clean_text(q['question'])\n",
    "    total_words.update(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = set()\n",
    "for rel in relation2id:\n",
    "    cleaned = clean_text(rel)\n",
    "    total_words.update(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(total_words & set(word2id.keys())) / len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actors',\n",
       " 'by',\n",
       " 'directed',\n",
       " 'genre',\n",
       " 'has',\n",
       " 'imdb',\n",
       " 'in',\n",
       " 'language',\n",
       " 'rating',\n",
       " 'release',\n",
       " 'starred',\n",
       " 'tags',\n",
       " 'votes',\n",
       " 'written',\n",
       " 'year'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words = set(word2id.keys()) | total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/metaqa/3hop/vocab_new.txt', 'w') as f:\n",
    "    for e in new_words:\n",
    "        f.writelines(e + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the films that share actors with the film Dil Chahta Hai were released in which years ?',\n",
       " ['starred_actors', 'starred_actors', 'release_year'])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "train_data[idx]['question'], train_data[idx]['rel_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_imdb_rating': 0,\n",
       " 'starred_actors': 1,\n",
       " 'in_language': 2,\n",
       " 'has_imdb_votes': 3,\n",
       " 'has_tags': 4,\n",
       " 'written_by': 5,\n",
       " 'directed_by': 6,\n",
       " 'has_genre': 7,\n",
       " 'release_year': 8}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt = use_helper.UseVector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for initialize graph:  1.2292625904083252\n",
      "Time for init session:  1.1187412738800049\n"
     ]
    }
   ],
   "source": [
    "rel_emb = {r: cvt.get_vector(r) for r in relation2id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 91.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'has_genre', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'starred_actors'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'release_year'}\n",
      "rel: {'has_tags', 'directed_by'} ,answers: {'in_language', 'directed_by'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'written_by', 'starred_actors'}\n",
      "rel: {'starred_actors', 'has_tags'} ,answers: {'starred_actors', 'directed_by'}\n",
      "0.4090909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "recall = 0.0\n",
    "total = 0\n",
    "for idx in tqdm(range(len(train_data)//10000)):\n",
    "    q_emb = cvt.get_vector(train_data[idx]['question'])\n",
    "    rel2score = dict()\n",
    "    for rel, remb in rel_emb.items():\n",
    "        qemb = torch.tensor(q_emb)\n",
    "        remb = torch.tensor(remb)\n",
    "        rel2score[rel] = torch.sigmoid(remb @ qemb.T).item()\n",
    "    rels = set(map(lambda x: x[0], list(sorted(rel2score.items(), key=lambda x: x[1], reverse=True)[:2])))\n",
    "    answers = set(train_data[idx]['rel_path'])\n",
    "    recall += len(rels & answers) / len(answers)\n",
    "    total += 1\n",
    "    print('rel:', rels, ',answers:', answers)\n",
    "print(recall / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embedding similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = len(word2id)\n",
    "word_dim = 100\n",
    "word_embedding = nn.Embedding(num_embeddings=num_word + 1, embedding_dim=word_dim, padding_idx=num_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.weight = nn.Parameter(\n",
    "                torch.from_numpy(np.pad(np.load('datasets/metaqa/3hop/word_emb_100d.npy'), ((0, 1), (0, 0)), 'constant')).type(\n",
    "                    'torch.FloatTensor'))\n",
    "word_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the films that share actors with the film Dil Chahta Hai were released in which years ?\n",
      "['starred_actors', 'starred_actors', 'release_year']\n",
      "has_imdb_rating\n",
      "starred_actors\n",
      "in_language\n",
      "has_imdb_votes\n",
      "has_tags\n",
      "written_by\n",
      "directed_by\n",
      "has_genre\n",
      "release_year\n",
      "rel: has_imdb_rating , similarity: 0.6172443628311157\n",
      "rel: starred_actors , similarity: 0.9318690896034241\n",
      "rel: in_language , similarity: 0.6172443628311157\n",
      "rel: has_imdb_votes , similarity: 0.6172443628311157\n",
      "rel: has_tags , similarity: 0.6172443628311157\n",
      "rel: written_by , similarity: 0.9073621034622192\n",
      "rel: directed_by , similarity: 0.9525083899497986\n",
      "rel: has_genre , similarity: 0.6172443628311157\n",
      "rel: release_year , similarity: 0.9408514499664307\n"
     ]
    }
   ],
   "source": [
    "idx_q = 0\n",
    "q = train_data[idx_q]\n",
    "print(q['question'])\n",
    "print(q['rel_path'])\n",
    "for r in relation2id:\n",
    "    print(r)\n",
    "rels = list(relation2id.keys())\n",
    "rel_emb = word_embedding\n",
    "for rel in rels:\n",
    "    rel_spt = clean_text(rel)\n",
    "    rel_spt = list(filter(lambda x: x not in ('has', 'by', 'in', 'the'), rel_spt))\n",
    "    q_spt = clean_text(q['question'])\n",
    "    q_spt = list(filter(lambda x: x not in ('has', 'which', 'by', 'with', 'were', 'that', 'in', 'the'), q_spt))\n",
    "    similarity = []\n",
    "    rel_emb = []\n",
    "    for rel_word in rel_spt:\n",
    "        if rel_word in word2id:\n",
    "            rel_word_emb = word_embedding(torch.tensor(word2id[rel_word], dtype=torch.int64))\n",
    "        else:\n",
    "            rel_word_emb = word_embedding(torch.tensor(word2id['__unk__'], dtype=torch.int64))\n",
    "        rel_emb.append(rel_word_emb)\n",
    "    rel_emb = sum(rel_emb) / len(rel_emb)\n",
    "    for word in q_spt:\n",
    "        word_emb = word_embedding(torch.tensor(word2id[word], dtype=torch.int64))\n",
    "        similarity.append(torch.sigmoid(rel_emb @ word_emb.T / np.sqrt(word_dim)).item())\n",
    "#         print('%s <--> %s, similarity: %.8f' % (word, rel_word, similarity[-1]))\n",
    "    print('rel:', rel, ', similarity:', np.max(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
